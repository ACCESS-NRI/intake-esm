{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "## Motivation\n",
    "\n",
    "\n",
    "Project efforts such as the [Coupled Model Intercomparison Project (CMIP)](https://www.wcrp-climate.org/wgcm-cmip) and the [Community Earth System Model (CESM) Large Ensemble Project](http://www.cesm.ucar.edu/projects/community-projects/LENS/) produce a huge of amount climate data persisted on tape, disk storage, object storage components across multiple (in the order of ~ 300,000) data assets. These data assets are stored in netCDF and more recently [Zarr](https://zarr.readthedocs.io/en/stable/) formats. Finding, investigating, loading these assets into data array containers such as xarray can be a daunting task due to the large number of files a user may be interested in. Intake-esm aims to address these issues by providing necessary functionality for searching, discovering, data access/loading. \n",
    "\n",
    "## Overview \n",
    "\n",
    "`intake-esm` is a data cataloging utility built on top of [intake](https://github.com/intake/intake), [pandas](https://pandas.pydata.org/), and [xarray](https://xarray.pydata.org/en/stable/), and it's pretty awesome! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening a collection\n",
    "\n",
    "First step is to point ``intake-esm`` to an ESM (Earth System Model) collection definition file, which is a JSON file that conforms to the [ESM Collection Specification](https://github.com/NCAR/esm-collection-spec). The collection JSON file can be stored on a local filesystem or can be hosted on a remote server. When provided a link/path to an esm collection file, `intake-esm` establishes a link to a database (CSV file) that contains assets (e.g. file) locations and associated metadata (i.e., which experiement, model, the come from). \n",
    "\n",
    "\n",
    "For demonstration purposes, we will be using the CMIP6 data hosted in Pangeo's Google Storage. For this data collection, we will be using the Pangeo cloud collection file stored [here](https://raw.githubusercontent.com/NCAR/intake-esm-datastore/master/catalogs/pangeo-cmip6.json).\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** \n",
    "    \n",
    "\n",
    "More collection examples are available in [intake-esm-datastore](https://github.com/NCAR/intake-esm-datastore) GitHub repository. \n",
    "    \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `intake-esm` is an `intake` plugin, the plugin automatically appears in the set of known\n",
    "plugins in the intake registry, and an associated ``intake.open_esm_datastore`` function is created at import time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intake.registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now open a collection catalog for CMIP6 data residing in Pangeo's Google storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/NCAR/intake-esm-datastore/master/catalogs/pangeo-cmip6.json\"\n",
    "col = intake.open_esm_datastore(url)\n",
    "col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `intake-esm` is build on top of [pandas](https://pandas.pydata.org/pandas-docs/stable), it is possible to view the `pandas.DataFrame` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to interact with the `DataFrame`; for instance, we can see what the \"attributes\" of the datasets are by printing the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col.df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search and discovery\n",
    "\n",
    "### Finding unique entries\n",
    "Let's query the data to see what models (\"source_id\"), experiments (\"experiment_id\") and temporal frequencies (\"table_id\") are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint \n",
    "uni_dict = col.unique(['source_id', 'experiment_id', 'table_id'])\n",
    "pprint.pprint(uni_dict, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for specific datasets\n",
    "\n",
    "\n",
    "One of the features supported in ``intake-esm`` is querying the collection catalog.\n",
    "This feature is provided via the ``search()`` method. The ``search()`` method allows the user to\n",
    "specify a query by using keyword arguments. This method returns a subset of the collection catalog\n",
    "with all the entries that match the provided query.\n",
    "\n",
    "Let's find all the dissolved oxygen data at annual frequency from the ocean for the `historical` and `ssp585` experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cat = col.search(experiment_id=['historical', 'ssp585'], table_id='Oyr', variable_id='o2', \n",
    "                 grid_label='gn')\n",
    "cat.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cat.df), len(col.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get summary information by using `.nunique()` and `.unique()` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(cat.unique(columns=[\"source_id\", \"activity_id\"]), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "The best part about `intake-esm` is that it enables loading data directly into an [xarray.Dataset](http://xarray.pydata.org/en/stable/api.html#dataset).\n",
    "\n",
    "Note that data on the cloud are in \n",
    "[zarr](https://zarr.readthedocs.io/en/stable/) and data on \n",
    "[glade](https://www2.cisl.ucar.edu/resources/storage-and-file-systems/glade-file-spaces) are stored as \n",
    "[netCDF](https://www.unidata.ucar.edu/software/netcdf/) files. This is opaque to the user!\n",
    "\n",
    "`intake-esm` has rules for aggegating datasets; these rules are defined in the collection-specification file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_dict = cat.to_dataset_dict(zarr_kwargs={'consolidated': True, 'decode_times': False}, \n",
    "                                cdf_kwargs={'chunks': {}, 'decode_times': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dset_dict` is a dictionary of `xarray.Dataset`'s; its keys are constructed to refer to compatible groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access a particular dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = dset_dict['CMIP.CCCma.CanESM5.historical.Oyr.gn']\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a quick plot for a slice of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "ds.o2.isel(time=0, lev=0, member_id=5).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can execute more searches against the original catalog and/or against a subset of the original catalog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cat_fx = col.search(table_id='Ofx', grid_label='gn',\n",
    "                    variable_id='volcello')\n",
    "cat_fx.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "a = cat_fx.search(source_id=\"CESM2\")\n",
    "a.df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cat_fx.df), len(a.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using custom preprocessing functions\n",
    "When comparing many models it is often necessary to preprocess (e.g. rename certain variables) them before running some analysis step. The `preprocess` argument lets the user pass a function, which is executed for each loaded zstore before merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pp = col.search(experiment_id=['historical'], table_id='Oyr', variable_id='o2', \n",
    "                    grid_label='gn', source_id=['IPSL-CM6A-LR', 'CanESM5'],\n",
    "                    member_id='r10i1p1f1')\n",
    "cat_pp.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the example\n",
    "dset_dict_raw = cat_pp.to_dataset_dict(zarr_kwargs={'consolidated': True})\n",
    "for k,ds in dset_dict_raw.items():\n",
    "    print(k)\n",
    "    print(list(ds.dims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both models follow a different naming scheme. We can define a little helper function and pass it to ` .to_dataset_dict` to fix this. For demonstration purposes we will focus on the vertical level dimension which is called `lev` in `CanESM5` and `olevel` in `IPSL-CM6A-LR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_func(ds):\n",
    "    ds = ds.copy()\n",
    "    # a short example \n",
    "    if 'olevel' in ds.dims: \n",
    "        ds = ds.rename({'olevel':'lev'})\n",
    "    return ds\n",
    "        \n",
    "\n",
    "dset_dict_fixed = cat_pp.to_dataset_dict(zarr_kwargs={'consolidated': True}, preprocess=helper_func)\n",
    "for k,ds in dset_dict_fixed.items():\n",
    "    print(k)\n",
    "    print(list(ds.dims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was just an example for one dimension. Check out [cmip6-preprocessing](https://github.com/jbusecke/cmip6_preprocessing/blob/master/notebooks/tutorial_intake_esm_preprocessing.ipynb) for a full renaming function for all available CMIP6 models and some other utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -d -iv -p intake-esm -m -g -h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
